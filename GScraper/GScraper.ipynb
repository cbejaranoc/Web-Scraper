{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d20c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GScraper Code - Spacecrafts - DO NOT TOUCH\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from operator import itemgetter\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, filename='debug.log')\n",
    "\n",
    "# Create a requests session object to reuse connections and headers\n",
    "session = requests.Session()\n",
    "\n",
    "def download_table() -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Downloads the HTML content of the web page containing the launch data table and returns it as a BeautifulSoup object.\n",
    "    Uses the requests.Session object to make the request and the lxml parser instead of html.parser.\n",
    "    \"\"\"\n",
    "    url = \"https://space.skyrocket.de/doc_chr/lau2023.htm\"\n",
    "    response = session.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "def load_table(start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> list:\n",
    "    \"\"\"\n",
    "    Loads the data from the launch table and returns it as a list of dictionaries.\n",
    "    Filters the launches by start_date and end_date if provided.\n",
    "    \"\"\"\n",
    "    soup = download_table()\n",
    "\n",
    "    table = soup.find(\"table\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    if table:\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "\n",
    "            if not cells:\n",
    "                continue\n",
    "\n",
    "            if cells[0].select(\"#Planned\"):\n",
    "                continue\n",
    "\n",
    "            id_ = cells[0]\n",
    "            date = cells[1]\n",
    "            payloads = cells[2]\n",
    "\n",
    "            # Remove unnecessary spans\n",
    "            for span in payloads.find_all(\"span\", class_=\"compact\"):\n",
    "                span.decompose()\n",
    "            for span in payloads.find_all(\"span\", class_=[\"detailed\", \"indent\"]):\n",
    "                span.unwrap()\n",
    "\n",
    "            vehicle = cells[3]\n",
    "            site = cells[4]\n",
    "            remark = cells[5]\n",
    "\n",
    "            for payload in payloads.contents:\n",
    "                payload_name = ''\n",
    "                payload_url = None\n",
    "\n",
    "                if payload.name == 'br':\n",
    "                    continue\n",
    "\n",
    "                elif payload.name == 'a':\n",
    "                    payload_url = payload['href']\n",
    "                    payload_name = payload.text\n",
    "\n",
    "                elif payload.name is None and len(payload.text) > 2:\n",
    "                    payload_name = payload.text\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Skip dates with \"X\", \"Q\", or \"?\"\n",
    "                try:\n",
    "                    launch_date = datetime.strptime(date.text.strip(), '%d.%m.%Y')\n",
    "                    if any(x in date.text for x in ['X', 'Q', '?']):\n",
    "                        continue\n",
    "                    elif start_date and end_date and (launch_date < start_date or launch_date > end_date):\n",
    "                        continue\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                data.append({\n",
    "                    'id': id_.text.strip(),\n",
    "                    'date': date.text.strip(),\n",
    "                    'payload_name': payload_name.strip(),\n",
    "                    'payload_url': payload_url,\n",
    "                    'vehicle': vehicle.text.strip(),\n",
    "                    'site': site.text.strip(),\n",
    "                    'remark': remark.text.strip()\n",
    "                })\n",
    "\n",
    "    return data\n",
    "\n",
    "def download_details(url: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Downloads and returns the details of the payloads from the given URL.\n",
    "    \"\"\"\n",
    "    url = f\"https://space.skyrocket.de/doc_chr/{url}\"\n",
    "\n",
    "    # Use the session object to make the request\n",
    "    response = session.get(url)\n",
    "\n",
    "    # Use the lxml parser instead of html.parser\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "    return soup\n",
    "\n",
    "def load_details(url, payload_name, date):\n",
    "    soup = download_details(url)\n",
    "\n",
    "    table = soup.find(id='satlist')\n",
    "\n",
    "    if table is None:\n",
    "        print(f\"No table found for {url}\")\n",
    "        return {}\n",
    "\n",
    "    cospars = []\n",
    "\n",
    "    # Use CSS selectors instead of find_all\n",
    "    for row in table.select(\"tr\"):\n",
    "        cells = row.select(\"td\")\n",
    "\n",
    "        if not cells:\n",
    "            continue\n",
    "\n",
    "        sat_name = cells[0].get_text(strip=True)\n",
    "        cospar = cells[1]\n",
    "        sat_date = cells[2].get_text(strip=True)\n",
    "\n",
    "        cospars.append([sat_name, sat_date, cospar])\n",
    "\n",
    "    details = {\n",
    "        \"Country\": None,\n",
    "        \"Market Segment\": None,\n",
    "        \"Operator\": None,\n",
    "        \"Prime Manufacturer\": None,\n",
    "        \"Equipment\": None,\n",
    "        \"Configuration\": None,\n",
    "        \"Propulsion\": None,\n",
    "        \"Power\": None,\n",
    "        \"Design Life\": None,\n",
    "        \"Mass\": None,\n",
    "        \"Orbit Type\": None,\n",
    "        \"Cospar\": cospars\n",
    "    }\n",
    "\n",
    "    # Use CSS selectors instead of find\n",
    "    details[\"Country\"] = soup.select_one('#sdnat').get_text(strip=True) if soup.select_one('#sdnat') else \"None\"\n",
    "    details[\"Market Segment\"] = soup.select_one('#sdtyp').get_text(strip=True) if soup.select_one('#sdtyp') else \"None\"\n",
    "    details[\"Operator\"] = soup.select_one('#sdope').get_text(strip=True) if soup.select_one('#sdope') else \"None\"\n",
    "    details[\"Prime Manufacturer\"] = soup.select_one('#sdcon').get_text(strip=True) if soup.select_one('#sdcon') else \"None\"\n",
    "    details[\"Equipment\"] = soup.select_one('#sdequ').get_text(strip=True) if soup.select_one('#sdequ') else \"None\"\n",
    "    details[\"Configuration\"] = soup.select_one('#sdcnf').get_text(strip=True) if soup.select_one('#sdcnf') else \"None\"\n",
    "    details[\"Propulsion\"] = soup.select_one('#sdpro').get_text(strip=True) if soup.select_one('#sdpro') else \"None\"\n",
    "    details[\"Power\"] = soup.select_one('#sdpow').get_text(strip=True) if soup.select_one('#sdpow') else \"None\"\n",
    "    details[\"Design Life\"] = soup.select_one('#sdlif').get_text(strip=True) if soup.select_one('#sdlif') else \"None\"\n",
    "    details[\"Mass\"] = soup.select_one('#sdmas').get_text(strip=True) if soup.select_one('#sdmas') else \"None\"\n",
    "    details[\"Orbit Type\"] = soup.select_one('#sdorb').get_text(strip=True) if soup.select_one('#sdorb') else \"None\"\n",
    "\n",
    "    return details\n",
    "\n",
    "def match_cospar(payload_name: str, payload_date: str, cospar_list: list) -> str:\n",
    "    \"\"\"\n",
    "    Matches the payload name and date to the corresponding COSPAR number.\n",
    "    Returns the matched COSPAR number or None if no match is found.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for name, date, cospar in cospar_list:\n",
    "        match_name = fuzz.token_set_ratio(payload_name, name)\n",
    "        match_date = fuzz.token_set_ratio(payload_date, date)\n",
    "\n",
    "        if match_name == 100 and match_date == 100:\n",
    "            print(\"100% match\")\n",
    "            return cospar.get_text(strip=True)\n",
    "        if match_name >= 90 and match_date >= 90:\n",
    "            print(f\">90% match: {payload_name} match = {match_name}, {date} match = {match_date}\")\n",
    "            ret.append([match_name + match_date, cospar.get_text(strip=True)])\n",
    "        else:\n",
    "            print(f\"<90% match: {payload_name} match = {match_name}, {date} match = {match_date}\")\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No match for COSPAR\")\n",
    "        return None\n",
    "\n",
    "    return sorted(ret, key=itemgetter(0), reverse=True)[0][1]\n",
    "\n",
    "def create_csv(filename: str, launches: list, details: dict) -> None:\n",
    "    \"\"\"\n",
    "    Writes the scraped data to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Date', 'Spacecraft Name', 'URL', 'Vehicle Name', 'Launch Site', 'Remark', 'Country',\n",
    "                         'Market Segment', 'Operator', 'Prime Manufacturer', 'Equipment', 'Configuration', 'Propulsion',\n",
    "                        'Power', 'Design Life', 'Mass', 'Orbit','COSPAR', 'Date Created'])\n",
    "\n",
    "        for launch in launches:\n",
    "            try:\n",
    "                launch_details = details[launch['payload_url']]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "            # Date Formatting\n",
    "            parsed_date = datetime.strptime(launch['date'], '%d.%m.%Y').strftime('%m/%d/%Y')\n",
    "            cospar = None\n",
    "            country = 'None'\n",
    "            market_segment = 'None'\n",
    "            operator = 'None'\n",
    "            prime_manufacturer = 'None'\n",
    "            equipment = 'None'\n",
    "            configuration = 'None'\n",
    "            propulsion = 'None'\n",
    "            power = 'None'\n",
    "            design_life = 'None'\n",
    "            mass = 'None'\n",
    "            orbit_type = 'None'\n",
    "\n",
    "            if parsed_date and parsed_date <= datetime.now().strftime('%m/%d/%Y'):\n",
    "                try:\n",
    "                    cospar = match_cospar(launch['payload_name'], launch['date'], launch_details['Cospar'])\n",
    "                    print(\"COSPAR match:\", cospar)\n",
    "                except KeyError:\n",
    "                    print(\"ERROR: could not match COSPAR for:\", launch['payload_name'])\n",
    "\n",
    "                country = launch_details.get('Country', 'None')\n",
    "                market_segment = launch_details.get('Market Segment', 'None')\n",
    "                operator = launch_details.get('Operator', 'None')\n",
    "                prime_manufacturer = launch_details.get('Prime Manufacturer', 'None')\n",
    "                equipment = launch_details.get('Equipment', 'None')\n",
    "                configuration = launch_details.get('Configuration', 'None')\n",
    "                propulsion = launch_details.get('Propulsion', 'None')\n",
    "                power = launch_details.get('Power', 'None')\n",
    "                design_life = launch_details.get('Design Life', 'None')\n",
    "                mass = launch_details.get('Mass', 'None')\n",
    "                orbit_type = launch_details.get('Orbit Type', 'None')\n",
    "\n",
    "            writer.writerow([\n",
    "                parsed_date,\n",
    "                launch['payload_name'],\n",
    "                launch['payload_url'],\n",
    "                launch['vehicle'],\n",
    "                launch['site'],\n",
    "                launch['remark'],\n",
    "                country,\n",
    "                market_segment,\n",
    "                operator,\n",
    "                prime_manufacturer,\n",
    "                equipment,\n",
    "                configuration,\n",
    "                propulsion,\n",
    "                power,\n",
    "                design_life,\n",
    "                mass,\n",
    "                orbit_type,\n",
    "                cospar,\n",
    "                datetime.now().strftime('%m/%d/%Y %H:%M:%S') # Date Created\n",
    "            ])\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to scrape and write data to CSV file.\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime('02.01.2023', '%d.%m.%Y')\n",
    "    end_date = datetime.strptime('03.01.2023', '%d.%m.%Y')\n",
    "    launches = load_table(start_date=start_date, end_date=end_date)\n",
    "\n",
    "    urls = set(launch['payload_url'] for launch in launches)\n",
    "\n",
    "    details = {}\n",
    "\n",
    "    for launch in launches:\n",
    "        details[launch['payload_url']] = {}\n",
    "\n",
    "    for url in urls:\n",
    "        payload_name = None\n",
    "        date = None\n",
    "        for launch in launches:\n",
    "            if launch['payload_url'] == url:\n",
    "                payload_name = launch['payload_name']\n",
    "                date = launch['date']\n",
    "                break\n",
    "\n",
    "        print(f'Loading {url}')\n",
    "        details[url] = load_details(url, payload_name, date)\n",
    "        time.sleep(random.randint(1, 10))\n",
    "\n",
    "    create_csv('G_SC.csv', launches, details)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da39d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GScraper Code - Launch Events - DO NOT TOUCH\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from operator import itemgetter\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, filename='debug.log')\n",
    "\n",
    "# Create a requests session object to reuse connections and headers\n",
    "session = requests.Session()\n",
    "\n",
    "def download_table() -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Downloads the HTML content of the web page containing the launch data table and returns it as a BeautifulSoup object.\n",
    "    Uses the requests.Session object to make the request and the lxml parser instead of html.parser.\n",
    "    \"\"\"\n",
    "    url = \"https://space.skyrocket.de/doc_chr/lau2023.htm\"\n",
    "    response = session.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "def load_table(start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> list:\n",
    "    \"\"\"\n",
    "    Loads the data from the launch table and returns it as a list of dictionaries.\n",
    "    Filters the launches by start_date and end_date if provided.\n",
    "    \"\"\"\n",
    "    soup = download_table()\n",
    "\n",
    "    table = soup.find(\"table\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    if table:\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "\n",
    "            if not cells:\n",
    "                continue\n",
    "\n",
    "            if cells[0].select(\"#Planned\"):\n",
    "                continue\n",
    "\n",
    "            date = cells[1]\n",
    "            vehicle = cells[3]\n",
    "            site = cells[4]\n",
    "\n",
    "            # Skip dates with \"X\", \"Q\", or \"?\"\n",
    "            try:\n",
    "                launch_date = datetime.strptime(date.text.strip(), '%d.%m.%Y')\n",
    "                if any(x in date.text for x in ['X', 'Q', '?']):\n",
    "                    continue\n",
    "                elif start_date and end_date and (launch_date < start_date or launch_date > end_date):\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            data.append({\n",
    "                'date': date.text.strip(),\n",
    "                'vehicle': vehicle.text.strip(),\n",
    "                'site': site.text.strip(),\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_csv(filename: str, launches: list, details: dict) -> None:\n",
    "    \"\"\"\n",
    "    Writes the scraped data to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Date', 'Launch Vehicle', 'Site'])\n",
    "\n",
    "        for launch in launches:\n",
    "            date_obj = datetime.strptime(launch['date'], '%d.%m.%Y')\n",
    "            date_str = datetime.strftime(date_obj, '%m/%d/%Y')\n",
    "            writer.writerow([date_str, launch['vehicle'], launch['site']])\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to scrape and write data to CSV file.\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime('02.01.2023', '%d.%m.%Y')\n",
    "    end_date = datetime.strptime('07.03.2023', '%d.%m.%Y')\n",
    "    launches = load_table(start_date=start_date, end_date=end_date)\n",
    "\n",
    "    create_csv('G_LE.csv', launches, {})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bae3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
